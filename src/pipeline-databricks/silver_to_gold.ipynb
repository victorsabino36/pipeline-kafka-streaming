{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda16844-f8d0-4624-9c43-97d8997ed5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_date, date_format, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2a59af-f771-451f-b9b9-3f7bf8173c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# path silver aws s3\n",
    "path_silver = \"s3://aws-data-lakehouse/silver/crypto/\"\n",
    "path_gold = \"s3://aws-data-lakehouse/gold/crypto/\"\n",
    "# read df_silver\n",
    "df_silver = spark.read.format(\"delta\").load(path_silver)\n",
    "#display df_silver\n",
    "display(df_silver.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341e65b1-9d17-43b3-9f3d-71dc78e26ed4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767458113084}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_silver.select(\"name\",\"current_price\",\"last_updated\") \\\n",
    "    .withColumn(\"date_hour_ref\",date_format(col(\"last_updated\"), \"yyyy-MM-dd HH\")) \\\n",
    "    .withColumn(\"processed_to_gold\", current_timestamp())\n",
    "\n",
    "df_sorted = df_filtered.select(\"name\",\"current_price\",\"date_hour_ref\").orderBy(\n",
    "   # col(\"name\").asc(), \n",
    "    col(\"date_hour_ref\").desc()\n",
    ")\n",
    "\n",
    "df_sorted.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(path_gold)\n",
    "\n",
    "display(df_sorted.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97400521-9538-4253-b673-e72f9199fcad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, avg, lag, round, when, max as max_\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ==========================================\n",
    "# 1. Escolhe a crypto para prever e traz media por hora\n",
    "# ==========================================\n",
    "\n",
    "df_btc = df_sorted.filter(col(\"name\") == \"Bitcoin\") \\\n",
    "    .groupBy(\"name\", \"date_hour_ref\") \\\n",
    "    .agg(avg(\"current_price\").alias(\"avg_price\"))\n",
    "\n",
    "# Comentário: Definimos a janela explicitamente para evitar o Warning de performance\n",
    "window_btc = Window.partitionBy(\"name\").orderBy(\"date_hour_ref\")\n",
    "\n",
    "df_features = df_btc.withColumn(\"lag_1\", lag(\"avg_price\", 1).over(window_btc)) \\\n",
    "                    .withColumn(\"lag_2\", lag(\"avg_price\", 2).over(window_btc)) \\\n",
    "                    .withColumn(\"lag_3\", lag(\"avg_price\", 3).over(window_btc)) \\\n",
    "                    .dropna()\n",
    "\n",
    "# ==========================================\n",
    "# 2. TREINAMENTO\n",
    "# ==========================================\n",
    "assembler = VectorAssembler(inputCols=[\"lag_1\", \"lag_2\", \"lag_3\"], outputCol=\"features\")\n",
    "df_ml_input = assembler.transform(df_features)\n",
    "\n",
    "rf_btc = RandomForestRegressor(featuresCol=\"features\", labelCol=\"avg_price\", numTrees=150)\n",
    "model_btc = rf_btc.fit(df_ml_input)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PREVISÃO DA PRÓXIMA HORA (T+1)\n",
    "# ==========================================\n",
    "# Pegamos o último registro real para servir de base para o futuro\n",
    "last_rec = df_features.orderBy(col(\"date_hour_ref\").desc()).limit(1)\n",
    "\n",
    "# Preparamos as colunas para o T+1 (Deslocamos os lags)\n",
    "last_btc_state = last_rec.select(\n",
    "    \"name\",\n",
    "    col(\"avg_price\").alias(\"preco_atual\"),\n",
    "    col(\"avg_price\").alias(\"lag_1\"), \n",
    "    col(\"lag_1\").alias(\"lag_2\"),\n",
    "    col(\"lag_1\").alias(\"lag_3\"), # Simplificação para o próximo passo\n",
    "    col(\"date_hour\").alias(\"ultima_referencia_s3\")\n",
    ")\n",
    "\n",
    "# Comentário: REAPLICAMOS o assembler para criar a coluna 'features' necessária pelo modelo\n",
    "df_forecast_input = assembler.transform(last_btc_state)\n",
    "predictions = model_btc.transform(df_forecast_input)\n",
    "\n",
    "# ==========================================\n",
    "# 4. CÁLCULO DE VARIAÇÃO % E TENDÊNCIA\n",
    "# ==========================================\n",
    "df_final_percent = predictions.withColumn(\n",
    "    \"variacao_percentual\", \n",
    "    round(((col(\"prediction\") - col(\"preco_atual\")) / col(\"preco_atual\")) * 100, 4)\n",
    ").select(\n",
    "    \"name\",\n",
    "    \"ultima_referencia_s3\",\n",
    "    col(\"preco_atual\").cast(\"decimal(18,2)\"),\n",
    "    col(\"prediction\").alias(\"previsao_proxima_hora\").cast(\"decimal(18,2)\"),\n",
    "    col(\"variacao_percentual\"),\n",
    "    when(col(\"variacao_percentual\") > 0, \"⬆️ SUBIR\")\n",
    "    .otherwise(\"⬇️ CAIR\").alias(\"tendencia\")\n",
    ")\n",
    "\n",
    "\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(path_silver)\n",
    "\n",
    "display(df_final_percent)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
