{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db5d5a7-90d5-4f8c-8cec-cc9d5c3a6fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784d0196-72ff-44d5-a506-c072374eb7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comentário: Define o caminho que o erro apontou como já existente\n",
    "path_raw = \"s3://aws-data-lakehouse/raw/kafka-crypto\"  \n",
    "path_bronze = \"s3://aws-data-lakehouse/bronze/kafka-crypto\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23abd52f-5f19-404b-8bf6-c96848b8e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comentário: Verifica se a pasta '_spark_metadata' e os primeiros arquivos JSON surgiram\n",
    "display(dbutils.fs.ls(\"s3://aws-data-lakehouse/raw/kafka-crypto/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329f3538-5557-4160-998b-0a4cea9b0206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DEFINIÇÃO DO SCHEMA MANUAL (EVITA INFERÊNCIA)\n",
    "# ==========================================\n",
    "# Forçamos tipos String para timestamps para garantir a leitura sem erros de conversão inicial\n",
    "schema_final = StructType([\n",
    "    StructField(\"topic\", StringType(), True),\n",
    "    StructField(\"partition\", LongType(), True),\n",
    "    StructField(\"offset\", LongType(), True),\n",
    "    StructField(\"kafka_timestamp\", StringType(), True),\n",
    "    StructField(\"ingested_at\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"current_price\", DoubleType(), True),\n",
    "    StructField(\"last_updated\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 2. LEITURA ROBUSTA NO UNITY CATALOG\n",
    "# ==========================================\n",
    "# Usamos o caminho com wildcard /*.json para garantir que não estamos tentando ler a pasta _spark_metadata\n",
    "path_raw_glob = \"s3://aws-data-lakehouse/raw/kafka-crypto/*.json\"\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .schema(schema_final) \\\n",
    "    .json(path_raw_glob)\n",
    "\n",
    "# Comentário: Filtramos registros onde o 'id' é nulo (arquivos vazios ou corrompidos)\n",
    "df_clean = df_raw.filter(col(\"id\").isNotNull())\n",
    "\n",
    "# Comentário: Adição de metadados de auditoria\n",
    "df_bronze = df_clean.select(\n",
    "    \"*\",\n",
    "    col(\"_metadata.file_path\").alias(\"origin_file\")\n",
    ").withColumn(\"processed_to_bronze\", current_timestamp())\n",
    "\n",
    "# ==========================================\n",
    "# 3. ESCRITA DELTA\n",
    "# ==========================================\n",
    "df_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(path_bronze)\n",
    "\n",
    "# ==========================================\n",
    "# 4. VALIDAÇÃO REAL\n",
    "# ==========================================\n",
    "total_rows = spark.read.format(\"delta\").load(path_bronze).count()\n",
    "print(f\"✅ Sucesso! {total_rows} linhas processadas na Bronze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0f7f60-5601-4e7a-9790-52f86e674f42",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767456793533}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma visão temporária na memória para permitir consultas SQL\n",
    "df_bronze.createOrReplaceTempView(\"v_bronze\")\n",
    "\n",
    "# Executa a query SQL sobre a visão criada\n",
    "df_ultimos = spark.sql(\"\"\"\n",
    "    SELECT * FROM v_bronze \n",
    "    ORDER BY ingested_at desc \n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(df_ultimos)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
