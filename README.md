üöÄ Crypto Streaming Pipeline: Kafka, Spark, AWS S3 & Databricks
üìù Objetivo
Este projeto implementa uma arquitetura de dados fim-a-fim para monitoramento de criptoativos em tempo real. O pipeline consome dados de uma API de mercado a cada 1 minuto, processa o fluxo via Kafka e distribui os dados para duas frentes:

Processamento Anal√≠tico: Persist√™ncia no Amazon S3 e transforma√ß√µes via Spark/Databricks (Arquitetura Medallion).

Monitoramento Real-time: Dashboard interativo em Streamlit para visualiza√ß√£o imediata da oscila√ß√£o de mercado.

üèó Arquitetura e Tecnologias
Ingestion: Python Producer coletando dados de APIs e enviando para o Kafka (Orquestrado por Strimzi Operator no Kubernetes).

Real-Time: Streamlit consumindo t√≥picos para dashboards de baixa lat√™ncia e indicadores de varia√ß√£o di√°ria.

Storage & ETL: Apache Spark persistindo dados brutos no Amazon S3, seguidos por transforma√ß√µes em camadas (Bronze, Silver e Gold) no Databricks.

Intelligence: Camada Gold alimentando modelos de Machine Learning para predi√ß√£o de pre√ßos e tend√™ncias.

Infrastructure: Docker e Kubernetes (K8s) garantindo a containeriza√ß√£o e resili√™ncia dos servi√ßos.

üìÇ Estrutura do Reposit√≥rio
Plaintext

‚îú‚îÄ‚îÄ docs/               # Documenta√ß√£o e diagramas de arquitetura
‚îú‚îÄ‚îÄ k8s/                # Manifestos Kubernetes (YAML)
‚îÇ   ‚îú‚îÄ‚îÄ kafka/          # Configura√ß√µes do Cluster Kafka e T√≥picos
‚îÇ   ‚îú‚îÄ‚îÄ producer/       # Deployment do Producer no cluster
‚îÇ   ‚îî‚îÄ‚îÄ strimzi/        # Instala√ß√£o do Strimzi Operator
‚îú‚îÄ‚îÄ src/                # C√≥digo fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ consumer/       # Consumers Spark e l√≥gica de ingest√£o S3
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/      # Aplica√ß√£o Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ databricks/     # Notebooks de transforma√ß√£o (Bronze -> Gold)
‚îÇ   ‚îî‚îÄ‚îÄ producer/       # Script Python de coleta da API
‚îî‚îÄ‚îÄ requirements.txt    # Depend√™ncias do projeto
--

# üõ† Guia de Configura√ß√£o (macOS)

1. Prepara√ß√£o do Ambiente Local
Ferramentas de Compila√ß√£o: xcode-select --install

Docker Desktop: Instale a vers√£o oficial e habilite o Kubernetes nas configura√ß√µes.

Resources: Aloque no m√≠nimo 4 CPUs e 8GB de RAM no Docker para suportar o cluster Kafka.

2. Configura√ß√£o Cloud (AWS & Databricks)
AWS S3: Crie um bucket com as pastas raw/, bronze/, silver/ e gold/. Configure um usu√°rio IAM com permiss√£o AmazonS3FullAccess.

Databricks: Configure um cluster (Spark 3.5+) e adicione suas AWS_ACCESS_KEY e AWS_SECRET_KEY nas configura√ß√µes de Spark para montagem do bucket.

3. Setup do Projeto
Bash

# Ativa√ß√£o do ambiente
cd pipeline-kafka-streaming
python3 -m venv venv
source venv/bin/activate

# Instala√ß√£o de depend√™ncias
pip install -r requirements.txt
Crie um arquivo .env na raiz:

Snippet de c√≥digo

AWS_ACCESS_KEY=SUA_CHAVE
AWS_SECRET_KEY=SEU_SECRET
DATABRICKS_TOKEN=SEU_TOKEN
4. Build e Deploy (Docker & K8s)
Bash

# Build das imagens locais
docker build -t crypto-producer:latest -f src/producer/Dockerfile .
docker build -t spark-crypto-consumer:latest -f src/consumer/Dockerfile .

# Instala√ß√£o do Strimzi e Kafka
kubectl create namespace kafka
kubectl apply -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka
kubectl apply -f k8s/kafka/cluster.yaml -n kafka
üèÉ Execu√ß√£o do Pipeline
T√∫neis de Conex√£o:

Bash

kubectl port-forward svc/crypto-cluster-kafka-external-bootstrap -n kafka 9094:9094 & 
kubectl port-forward svc/crypto-cluster-kafka-nodes-0 -n kafka 9095:9094
Dashboard:

Bash

streamlit run src/dashboard/dashboard.py
Processamento Databricks: Execute os notebooks em ordem (Bronze -> Silver -> Gold) para processar os dados hist√≥ricos.