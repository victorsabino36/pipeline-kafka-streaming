# ðŸš€ Crypto Streaming Pipeline: Kafka, Spark, AWS S3 & Databricks

## ðŸ“ Objetivo
Este projeto implementa uma arquitetura de dados fim-a-fim para monitoramento de criptoativos em tempo real. O pipeline consome dados de uma API de mercado em intervalos de 1 minuto, processa o fluxo via Kafka e distribui os dados para duas frentes distintas:

- **Processamento AnalÃ­tico**: PersistÃªncia no Amazon S3 e transformaÃ§Ãµes distribuÃ­das via Spark/Databricks seguindo a Arquitetura Medallion.
- **Monitoramento Real-time**: Dashboard interativo em Streamlit para visualizaÃ§Ã£o imediata da volatilidade do mercado.

## ðŸ— Arquitetura e Tecnologias
- **Ingestion**: Producer em Python coletando dados de APIs e enviando para o Kafka (orquestrado pelo Strimzi Operator no Kubernetes).
- **Real-Time**: Streamlit para consumo de tÃ³picos com baixa latÃªncia e indicadores de variaÃ§Ã£o.
- **Storage & ETL**: Apache Spark para persistÃªncia no S3 e processamento em camadas (Bronze, Silver e Gold) no Databricks.
- **Infrastructure**: Docker e Kubernetes (K8s) garantindo portabilidade e resiliÃªncia.

## ðŸ“‚ Estrutura do RepositÃ³rio
```
â”œâ”€â”€ docs/               # DocumentaÃ§Ã£o tÃ©cnica e diagramas de arquitetura
â”œâ”€â”€ k8s/                # Manifestos Kubernetes (Kafka, Producer, Strimzi)
â”‚   â”œâ”€â”€ kafka/          # ConfiguraÃ§Ãµes do Cluster Kafka e TÃ³picos
â”‚   â”œâ”€â”€ producer/       # Deployment do Producer no cluster
â”‚   â””â”€â”€ strimzi/        # InstalaÃ§Ã£o do Strimzi Operator
â”œâ”€â”€ src/                # CÃ³digo-fonte da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ consumer/       # Consumers Spark e lÃ³gica de escrita no S3
â”‚   â”œâ”€â”€ dashboard/      # AplicaÃ§Ã£o Streamlit
â”‚   â”œâ”€â”€ databricks/     # Notebooks de transformaÃ§Ã£o (Bronze -> Gold)
â”‚   â””â”€â”€ producer/       # Script Python de coleta da API
â””â”€â”€ requirements.txt    # DependÃªncias do projeto
```

## ðŸ› ï¸ Guia de ConfiguraÃ§Ã£o (macOS)

### 1. PreparaÃ§Ã£o do Ambiente Local
- **Ferramentas de CompilaÃ§Ã£o**: Instale o Xcode Command Line Tools:
```bash
  xcode-select --install
```
- **Docker Desktop**: Instale a versÃ£o oficial e habilite o Kubernetes nas configuraÃ§Ãµes.
- **Recursos**: Aloque no mÃ­nimo 4 CPUs e 4GB de RAM no Docker para suportar o cluster Kafka.

### 2. ConfiguraÃ§Ã£o do Storage AWS
- No Amazon S3, crie um bucket com as pastas: `raw/`, `bronze/`, `silver/` e `gold/`.
- Configure um usuÃ¡rio IAM com a polÃ­tica `AmazonS3FullAccess` para as chaves de acesso.

### 3. Setup do CÃ³digo e DependÃªncias
```bash
# Navega atÃ© o diretÃ³rio raiz e isola o ambiente Python
cd pipeline-kafka-streaming
python3 -m venv venv
source venv/bin/activate

# InstalaÃ§Ã£o das bibliotecas necessÃ¡rias
pip install -r requirements.txt
```

### 4. ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente
Crie um arquivo `.env` na raiz do projeto:
```
AWS_ACCESS_KEY=SUA_CHAVE_AQUI
AWS_SECRET_KEY=SEU_SECRET_AQUI
DATABRICKS_TOKEN=SEU_TOKEN_AQUI
```

## ðŸ³ Build e Deploy (Docker & K8s)

### 1. ConstruÃ§Ã£o das Imagens
```bash
# Build das imagens locais para o Producer e Consumer Spark
docker build -t crypto-producer:latest -f src/producer/Dockerfile .
docker build -t spark-crypto-consumer:latest -f src/consumer/Dockerfile .
```

### 2. Provisionamento do Cluster Kafka
```bash
# CriaÃ§Ã£o do namespace e instalaÃ§Ã£o do Strimzi Operator
kubectl create namespace kafka
kubectl apply -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka
kubectl apply -f k8s/kafka/cluster.yaml -n kafka
```

## ðŸƒ ExecuÃ§Ã£o do Pipeline

### 1. TÃºneis de ConexÃ£o (Port-Forward)
NecessÃ¡rio para expor o broker Kafka rodando internamente no cluster para o host local:
```bash
kubectl port-forward svc/crypto-cluster-kafka-external-bootstrap -n kafka 9094:9094 & 
kubectl port-forward svc/crypto-cluster-kafka-nodes-0 -n kafka 9095:9094
```

### 2. Dashboard Streamlit
```bash
# Inicializa a visualizaÃ§Ã£o em tempo real
streamlit run src/dashboard/dashboard.py
```

## ðŸ“Š Camada AnalÃ­tica (Databricks)
- **Cluster**: Configure um cluster (Spark 3.5+) e aponte a fonte de dados externa para o S3.
- **Notebooks**: Importe os scripts de `src/databricks/` e ajuste os caminhos para o bucket.
- **Fluxo**: ApÃ³s a escrita no S3 iniciar, execute os notebooks na ordem:
```
  raw_to_bronze >> bronze_to_silver >> silver_to_gold >> ml_predictions
```
